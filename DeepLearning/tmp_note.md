## Promt tunning
"Prompt tuning" 是一种轻量级的参数高效微调方法，它不需要修改整个大模型的权重，而是通过学习**少量的可训练向量（prompt embeddings）**来引导模型产生你想要的输出。它主要应用于像 GPT-3、T5、BERT 这样的预训练语言模型。

### 📌 基本概念
- 目标：让预训练模型在特定任务上表现更好，而不微调全部参数。

- 方法：插入一组可学习的嵌入向量（prompt tokens），前缀在输入前，让模型根据这些向量引导其行为。

### 优势：

- 可训练参数数量少（例如只训练几十个向量）

- 训练快、计算资源少

- 在多任务环境中灵活性强

## 🧠 InfoNCE
InfoNCE 是目前最有效且通用的对比学习损失，适用于点云与语言表示对齐。

## Multi-Head Attention


## Cross attention
- 多模态: 让一个模态的信息指导另一个模态的处理（例如： 使文本信息引导图像的特征提取）
- 
- 
