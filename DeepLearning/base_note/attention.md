# Attention
所谓 attention 就是要着重关注特征向量的某些位置 -> 给予该位置更多的注意力，即有更高的权重。

各种注意力机制无非都是计算出一组应用在特征向量上的权重，根据计算的方式不同可以划分为各种不同的方式。

## Self Attention
即通过自身的值，计算出一组 Q, K, V。

通过这个 Q, K 计算出一组权重作用在 V 上。
$$
\text{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

即只通过自己计算得到需要注意的特征位置。

## Multi-Head Attention

即多个slef-attention 并行计算，此处可以将矩阵进行拼接从而提高计算效率。

计算 Q, K, V 时可以直接将多个头的计算矩阵按列拼接在一起。

具体计算权重矩阵时可以reshape 成 (batch_size, h, seq_len, dk) 后计算。

最后拼接在一起即可。

## Cross attention
- 多模态: 让一个模态的信息指导另一个模态的处理（例如： 使文本信息引导图像的特征提取）
- 一般是使用目标信息作为 Q, 输入作为 K 和 V。

以文本理解图像为例：

文本理解图像的信息，例如 “图片中有几只猫？”。

文本信息作为查询 Q，图片信息作为 K 和 V。从图像中去搜索和提取信息。

以文本生成图像为例：

图像需要从文本提取信息，进行生成，例如“一位坐在樱花树下的女孩”。

图像作为查询 Q，从文本中搜素和提取信息。